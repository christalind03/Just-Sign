{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Important Notes\n",
    "This notebook was created for the project \"Just Sign\" which is able to detect various signing motions for the song \"Cupid\" by FIFTY FIFTY.\n",
    "\n",
    "Created with the help of Nicholas Renotte's tutorial on action recogniton [here](https://www.youtube.com/watch?v=doDUihpj6ro&t=2862s&ab_channel=NicholasRenotte) as well as ChatGPT's responses on integrating variable-length input in real-time scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installing and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe tensorflow sklearn opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data\n",
    "DATA_PATH = os.path.join(\"Training Data\")\n",
    "\n",
    "# Array of all actions to detect\n",
    "all_actions = np.array([\n",
    "#                         \"HOPELESS\",     # START HALF ONE\n",
    "#                         \"SWEETHEART\",  \n",
    "#                         \"ALL\", \n",
    "#                         \"ME\", \n",
    "#                         \"LIFE\", \n",
    "#                         \"COUPLE\", \n",
    "#                         \"SURROUND\", \n",
    "#                         \"ALL\", \n",
    "#                         \"TIME\", \n",
    "#                         \"I\", \n",
    "#                         \"GUESS\",  \n",
    "#                         \"MEANS\", \n",
    "#                         \"SOMETHING\", \n",
    "#                         \"WHY\", \n",
    "#                         \"FEEL\", \n",
    "#                         \"LONELY\", \n",
    "#                         \"WISH\", \n",
    "#                         \"FIND\", \n",
    "#                         \"LOVER\", \n",
    "#                         \"HUG\", \n",
    "#                         \"NOW\",           # START HALF TWO\n",
    "#                         \"CRY\", \n",
    "#                         \"ROOM\", \n",
    "#                         \"SKEPTICAL\",  \n",
    "#                         \"LOVE\", \n",
    "#                         \"BUT\", \n",
    "#                         \"STILL\", \n",
    "#                         \"MORE\", \n",
    "#                         \"GIVE\", \n",
    "#                         \"NEW\", \n",
    "#                         \"CHANCE\",  \n",
    "#                         \"CUPID\", \n",
    "#                         \"STUPID\", \n",
    "#                         \"HE\", \n",
    "#                         \"MAKE\", \n",
    "#                         \"THAT\", \n",
    "#                         \"NOT\", \n",
    "#                         \"REAL\", \n",
    "#                         \"DUMB\", \n",
    "                        \"NO_DETECTIONS\" \n",
    "                        ])\n",
    "\n",
    "# Each action has 35 videos consisting of 25 frames\n",
    "num_sequences = 35\n",
    "sequence_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for each action\n",
    "for action in all_actions:\n",
    "    for sequence in range(num_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Collect Keypoints Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    results = model.process(image)\n",
    "    \n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image,\n",
    "                              results.face_landmarks,\n",
    "                              mp_holistic.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color=(192, 255, 48), thickness=1, circle_radius=1), \n",
    "                              mp_drawing.DrawingSpec(color=(224, 224, 224), thickness=1, circle_radius=1))\n",
    "    \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image,\n",
    "                              results.pose_landmarks,\n",
    "                              mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(192, 255, 48), thickness=2, circle_radius=3), \n",
    "                              mp_drawing.DrawingSpec(color=(224, 224, 224), thickness=2, circle_radius=2))\n",
    "    \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image,\n",
    "                              results.left_hand_landmarks,\n",
    "                              mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(192, 255, 48), thickness=2, circle_radius=3), \n",
    "                              mp_drawing.DrawingSpec(color=(224, 224, 224), thickness=2, circle_radius=2))\n",
    "    \n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image,\n",
    "                              results.right_hand_landmarks,\n",
    "                              mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(192, 101, 21), thickness=2, circle_radius=3), \n",
    "                              mp_drawing.DrawingSpec(color=(224, 224, 224), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    left_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    return np.concatenate([face, pose, left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Setup the MediaPipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.75, min_tracking_confidence=0.75) as holistic:\n",
    "    # Loop through all the actions\n",
    "    for action in all_actions:\n",
    "        # Loop through all the videos\n",
    "        for sequence in range(num_sequences):\n",
    "            # Loop through all the frames\n",
    "            for frame_num in range(sequence_length):\n",
    "                # Read video feed\n",
    "                success, frame = video_capture.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                # Apply wait logic for recording\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, \"STARTING COLLECTION\", (120, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "\n",
    "                    cv2.putText(image, f\"Current Action: {action} - Video Number {sequence}\", (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Show to screen\n",
    "                    cv2.imshow(\"OpenCV Feed\", image)\n",
    "                    cv2.waitKey(1500)\n",
    "                    \n",
    "                else:\n",
    "                    cv2.putText(image, f\"Current Action: {action} - Video Number {sequence}\", (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Show to screen\n",
    "                    cv2.imshow(\"OpenCV Feed\", image)\n",
    "                      \n",
    "                # Save the keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                np_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(np_path, keypoints)\n",
    "                \n",
    "                # Quit application\n",
    "                if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "            \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(all_actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, sequences = [], []\n",
    "\n",
    "for action in all_actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        video = []\n",
    "        \n",
    "        for frame_num in range(sequence_length):\n",
    "            frame = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "            video.append(frame)\n",
    "            \n",
    "        sequences.append(video)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.backend import ctc_batch_cost\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, activation=\"relu\", input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation=\"relu\"))\n",
    "model.add(LSTM(64, return_sequences=False, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(all_actions.shape[0], activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=275, callbacks=[Tensorboard(log_dir=os.path.join(\"TensorBoard Logs\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ASL Recognition Model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Make Real-Time Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "model = tf.keras.models.load_model(\"ASL Recognition Model.h5\")\n",
    "\n",
    "# Setup the MediaPipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.75, min_tracking_confidence=0.75) as holistic:\n",
    "    while video_capture.isOpened():\n",
    "        # Read video feed\n",
    "        success, frame = video_capture.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Make predictions\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-25:]\n",
    "        \n",
    "        if len(sequence) == 25:\n",
    "            result = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(all_actions[np.argmax(result)])\n",
    "            predictions.append(np.argmax(result))\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "        # Quit application\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "            \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
